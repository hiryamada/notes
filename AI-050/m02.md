# モジュール2: Azure OpenAI Service で作業を開始する

https://learn.microsoft.com/ja-jp/training/modules/get-started-openai/

■Azure OpenAI Studioを利用する手順

- Azure OpenAI Serviceの利用を申請する
  - https://aka.ms/oaiapply
- 申請が承認されるまで待つ
- Azure OpenAI Serviceリソースを作成する
- モデルをデプロイする
- モデルを利用する
  - Azure OpenAI Studioから
  - 独自のアプリから

■モデルの種類

- GPT-4
- GPT 3.5
- Embedded
- DALL-E

■モデルの詳細

- gpt-35-turbo-16k
  - OpenAIが開発した大規模な言語モデルの一種。
  - 最大16,384個のトークン(テキストを分割した単位)を入力として受け付け可能
  - 日本語では約15,000文字程度に相当

■Azure OpenAI Studioとは

Azure OpenAI Service を操作するためのWeb管理画面。

https://oai.azure.com

■プレイグラウンド

- Azure OpenAI Studio のインターフェイス
- 独自のクライアント アプリケーションを開発することなく、デプロイされるモデルを実験するために使用できる

■トークン

Tokens are the basic units of a prompt - essentially words or word-parts in the text.

以下のサイトで、テキストを入力し、そのトークン数を計算できる。
https://platform.openai.com/tokenizer

■トークナイズ

https://cardinal-moon.hatenablog.com/entry/tokenize_and_subword

文章をトークンに分割すること。

```
私は日本人です。
↓
私/は/日本人/です/。
```

■サブワード分割（サブワード正則化）

ChatGPTにおけるサブワード分割とは、テキストを分割する単位のことです。
サブワード分割は、ニューラル言語処理の分野で広く使われている手法であり、機械翻訳や自然言語生成などに応用されています¹。
サブワード分割には、Byte Pair Encoding (BPE) やユニグラム言語モデルなどのアルゴリズムがあります²。
これらのアルゴリズムは、データ圧縮に基づいて、頻出する文字列や単語をトークンとして扱います³。
このようにして、テキストの長さを短くし、語彙数を減らすことができます⁴。

■BPE（Byte Pair Encoding）

ChatGPTは、Byte Pair Encoding (BPE) というアルゴリズムでトークンを生成しており、頻出する文字列や単語をトークンとして扱います²。

BPEとは、Byte Pair Encodingの略で、テキストを圧縮するアルゴリズムの一種です¹。BPEでは、テキストをバイト単位で分割し、最も頻出するバイトのペアを新しいバイトに置き換えていきます²。このようにして、テキストの長さを短くし、語彙数を減らすことができます³。

ChatGPTでは、BPEを用いてテキストをトークンに変換しています⁴。トークンとは、テキストを分割する単位のことで、文字や単語の一部などがトークンになります¹。BPEによってトークン化されたテキストは、ChatGPTのモデルに入力されて処理されます⁴。

ChatGPTでは、BPEというアルゴリズムでトークンを生成しています⁵。BPEでは、テキストをバイト単位で分割し、最も頻出するバイトのペアを新しいバイトに置き換えていきます⁶。例えば、「こんにちは」というテキストは、2つのトークンに分割されます。`こんにちは` -> `こんにちは` + `</s>`。`</s>`は文末を表す特殊なトークンです。一方、「Hello」というテキストは、5つのトークンに分割されます。`Hello` -> `H` + `el` + `lo` + `!` + `</s>`。このように、英語の場合は文字や単語の一部がトークンになることがあります。

■パラメーター

■温度（temperature）
